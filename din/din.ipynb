{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "'''\n",
    "功能：通用的模型内部模块，如参数初始化、embedding-pooling、target-attention、multi-heads self-attention等\n",
    "'''\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def emb_init(name, feat_num, embedding_size, zero_first_row=True, pre_trained=False, trained_emb_path=None):\n",
    "    if not pre_trained:\n",
    "        with tf.variable_scope(\"weight_matrix\"):\n",
    "            embeddings = tf.get_variable(name=name,\n",
    "                                         dtype=tf.float32,\n",
    "                                         shape=(feat_num, embedding_size),\n",
    "                                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        if zero_first_row:  # The first row of initialization is zero\n",
    "            embeddings = tf.concat((tf.zeros(shape=[1, embedding_size]), embeddings[1:]), 0)\n",
    "    else:\n",
    "        pass\n",
    "        with tf.variable_scope(\"pre-trained_weight_matrix\"):\n",
    "            load_emb = np.load(tf.gfile.GFile(trained_emb_path, \"rb\"))\n",
    "            embeddings = tf.constant(load_emb, dtype=tf.float32, name=name)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def nonzero_reduce_mean(emb):  # nonzero-mean-pooling\n",
    "    axis_2_sum = tf.reduce_sum(emb, axis=2)\n",
    "    multi_cate_nonzero = tf.count_nonzero(axis_2_sum, 1, keepdims=True, dtype=float)\n",
    "    multi_cate_sum = tf.reduce_sum(emb, axis=1)\n",
    "    reduce_mean_emb = tf.div_no_nan(multi_cate_sum, multi_cate_nonzero)\n",
    "    return reduce_mean_emb\n",
    "\n",
    "\n",
    "class InteractingLayer:  # multi-heads self-attention\n",
    "    def __init__(self, num_layer, w_name=\"default\", att_emb_size=32, seed=2020, head_num=3, use_res=1):\n",
    "        if head_num <= 0:\n",
    "            raise ValueError('head_num must be a int > 0')\n",
    "        self.num_layer = num_layer\n",
    "        self.att_emb_size = att_emb_size\n",
    "        self.seed = seed\n",
    "        self.head_num = head_num\n",
    "        self.use_res = use_res\n",
    "        self.w_name = w_name\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        input_shape = inputs.get_shape().as_list()\n",
    "        if len(input_shape) != 3:\n",
    "            raise ValueError(\"Unexpected inputs dimensions %d, expect to be 3 dimensions\" % len(input_shape))\n",
    "\n",
    "        embedding_size = int(input_shape[-1])\n",
    "        self.w_query = tf.get_variable(name=str(self.w_name) + str(self.num_layer) + '_query',\n",
    "                                       dtype=tf.float32,\n",
    "                                       shape=(embedding_size, self.att_emb_size * self.head_num),\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer(seed=self.seed))\n",
    "        self.w_key = tf.get_variable(name=str(self.w_name) + str(self.num_layer) + '_key',\n",
    "                                     dtype=tf.float32,\n",
    "                                     shape=(embedding_size, self.att_emb_size * self.head_num),\n",
    "                                     initializer=tf.contrib.layers.xavier_initializer(seed=self.seed + 1))\n",
    "        self.w_value = tf.get_variable(name=str(self.w_name) + str(self.num_layer) + '_value',\n",
    "                                       dtype=tf.float32,\n",
    "                                       shape=(embedding_size, self.att_emb_size * self.head_num),\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer(seed=self.seed + 2))\n",
    "        if self.use_res:\n",
    "            self.w_res = tf.get_variable(name=str(self.w_name) + str(self.num_layer) + '_res',\n",
    "                                         dtype=tf.float32,\n",
    "                                         shape=(embedding_size, self.att_emb_size * self.head_num),\n",
    "                                         initializer=tf.contrib.layers.xavier_initializer(seed=self.seed))\n",
    "\n",
    "        querys = tf.tensordot(inputs, self.w_query, axes=1)  # None F D*head_num\n",
    "        keys = tf.tensordot(inputs, self.w_key, axes=1)\n",
    "        values = tf.tensordot(inputs, self.w_value, axes=1)\n",
    "\n",
    "        # head_num None F D\n",
    "        querys = tf.stack(tf.split(querys, self.head_num, axis=2))\n",
    "        keys = tf.stack(tf.split(keys, self.head_num, axis=2))\n",
    "        values = tf.stack(tf.split(values, self.head_num, axis=2))\n",
    "\n",
    "        inner_product = tf.matmul(querys, keys, transpose_b=True)  # head_num None F F\n",
    "        # Scale\n",
    "        inner_product = inner_product / (keys.get_shape().as_list()[-1] ** 0.5)\n",
    "        # Activation\n",
    "        self.normalized_att_scores = tf.nn.softmax(inner_product)\n",
    "\n",
    "        result = tf.matmul(self.normalized_att_scores, values)  # head_num None F D\n",
    "        result = tf.concat(tf.split(result, self.head_num, axis=0), axis=-1)  # 1 None F D*head_num\n",
    "        result = tf.squeeze(result, axis=0)  # None F D*head_num\n",
    "\n",
    "        if self.use_res:\n",
    "            result += tf.tensordot(inputs, self.w_res, axes=1)\n",
    "        result = tf.nn.relu(result)\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "def attention(queries, keys, keys_length):  # target-attention\n",
    "    \"\"\"\n",
    "      queries:     [B, H] 前面的B代表的是batch_size，H代表向量维度。\n",
    "      keys:        [B, T, H] T是一个batch中，当前特征最大的长度，每个样本代表一个样本的特征\n",
    "      keys_length: [B]\n",
    "    \"\"\"\n",
    "    # H 每个query词的隐藏层神经元是多少，也就是H\n",
    "    queries_hidden_units = queries.get_shape().as_list()[-1]\n",
    "    # tf.tile为复制函数，1代表在B上保持一致，tf.shape(keys)[1] 代表在H上复制这么多次, 那么queries最终shape为(B, H*T)\n",
    "    queries = tf.tile(queries, [1, tf.shape(keys)[1]])\n",
    "    # queries.shape(B, T, H) 其中每个元素(T,H)代表T行H列，其中每个样本中，每一行的数据都是一样的\n",
    "    queries = tf.reshape(queries, [-1, tf.shape(keys)[1], queries_hidden_units])\n",
    "    # 下面4个变量的shape都是(B, T, H)，按照最后一个维度concat，所以shape是(B, T, H*4), 在这块就将特征中的每个item和目标item连接在了一起\n",
    "    din_all = tf.concat([queries, keys, queries - keys, queries * keys], axis=-1)\n",
    "    # (B, T, 80)\n",
    "    d_layer_1_all = tf.layers.dense(din_all, 80, activation=tf.nn.sigmoid, name='f1_att', reuse=tf.AUTO_REUSE)\n",
    "    # (B, T, 40)\n",
    "    d_layer_2_all = tf.layers.dense(d_layer_1_all, 40, activation=tf.nn.sigmoid, name='f2_att', reuse=tf.AUTO_REUSE)\n",
    "    # (B, T, 1)\n",
    "    d_layer_3_all = tf.layers.dense(d_layer_2_all, 1, activation=None, name='f3_att', reuse=tf.AUTO_REUSE)\n",
    "    # (B, 1, T)\n",
    "    # 每一个样本都是 [1,T] 的维度，和原始特征的维度一样，但是这时候每个item已经是特征中的一个item和目标item混在一起的数值了\n",
    "    d_layer_3_all = tf.reshape(d_layer_3_all, [-1, 1, tf.shape(keys)[1]])\n",
    "    outputs = d_layer_3_all\n",
    "    # Mask，每一行都有T个数字，keys_length长度为B，假设第1 2个数字是5,6，那么key_masks第1 2行的前5 6个数字为True\n",
    "    key_masks = tf.sequence_mask(keys_length, tf.shape(keys)[1])  # [B, T]\n",
    "    key_masks = tf.expand_dims(key_masks, 1)  # [B, 1, T]\n",
    "    # 创建一个和outputs的shape保持一致的变量，值全为1，再乘以(-2 ** 32 + 1)，所以每个值都是(-2 ** 32 + 1)\n",
    "    paddings = tf.ones_like(outputs) * (-2 ** 32 + 1)\n",
    "    outputs = tf.where(key_masks, outputs, paddings)  # [B, 1, T]\n",
    "\n",
    "    # Scale\n",
    "    outputs = outputs / (keys.get_shape().as_list()[-1] ** 0.5)  # T，根据特征数目来做拉伸\n",
    "    # Activation\n",
    "    outputs = tf.nn.softmax(outputs)  # [B, 1, T]\n",
    "    # Weighted sum\n",
    "    outputs = tf.matmul(outputs, keys)  # [B, 1, H]\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def attention_multi(queries, keys, keys_length):  # target-attention for multi-feats-queries\n",
    "    \"\"\"\n",
    "      queries:     [B, N, H] (e.g. N is the number of ads)\n",
    "      keys:        [B, T, H]\n",
    "      keys_length: [B]\n",
    "    \"\"\"\n",
    "    # H 每个query词的隐藏层神经元是多少，也就是H\n",
    "    queries_hidden_units = queries.get_shape().as_list()[-1]\n",
    "    # N\n",
    "    queries_nums = queries.get_shape().as_list()[1]\n",
    "    queries = tf.tile(queries, [1, 1, tf.shape(keys)[1]])\n",
    "    queries = tf.reshape(queries, [-1, queries_nums, tf.shape(keys)[1], queries_hidden_units])  # shape : [B, N, T, H]\n",
    "    max_len = tf.shape(keys)[1]\n",
    "    keys = tf.tile(keys, [1, queries_nums, 1])\n",
    "    keys = tf.reshape(keys, [-1, queries_nums, max_len, queries_hidden_units])  # shape : [B, N, T, H]\n",
    "    din_all = tf.concat([queries, keys, queries - keys, queries * keys], axis=-1)\n",
    "    d_layer_1_all = tf.layers.dense(din_all, 80, activation=tf.nn.sigmoid, name='f1_att', reuse=tf.AUTO_REUSE)\n",
    "    d_layer_2_all = tf.layers.dense(d_layer_1_all, 40, activation=tf.nn.sigmoid, name='f2_att', reuse=tf.AUTO_REUSE)\n",
    "    d_layer_3_all = tf.layers.dense(d_layer_2_all, 1, activation=None, name='f3_att', reuse=tf.AUTO_REUSE)  # [B, N, T, 1]\n",
    "    d_layer_3_all = tf.reshape(d_layer_3_all, [-1, queries_nums, 1, max_len])  # [B, N, 1, T]\n",
    "    outputs = d_layer_3_all\n",
    "    # Mask\n",
    "    key_masks = tf.sequence_mask(keys_length, max_len)  # [B, T]\n",
    "    key_masks = tf.tile(key_masks, [1, queries_nums])  # [B, N, T]\n",
    "    key_masks = tf.reshape(key_masks, [-1, queries_nums, 1, max_len])  # shape : [B, N, 1, T]\n",
    "    paddings = tf.ones_like(outputs) * (-2 ** 32 + 1)\n",
    "    outputs = tf.where(key_masks, outputs, paddings)  # [B, N, 1, T]\n",
    "\n",
    "    # Scale\n",
    "    outputs = outputs / (keys.get_shape().as_list()[-1] ** 0.5)\n",
    "\n",
    "    # Activation\n",
    "    outputs = tf.nn.softmax(outputs)  # [B, N, 1, T]\n",
    "    # Weighted sum\n",
    "    outputs = tf.matmul(outputs, keys)  # [B, N, 1, H]\n",
    "\n",
    "    # Pooling\n",
    "    # outputs = tf.reduce_mean(outputs, axis=1)  # [B, 1, H] mean-pooling\n",
    "    outputs = tf.reduce_sum(outputs, axis=1)  # [B, 1, H] sum-pooling\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "'''\n",
    "功能: 模型共用模块，如训练、预测、保存等\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def model_optimizer(params, mode, labels, out):\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=out)\n",
    "\n",
    "    else:\n",
    "        labels = tf.identity(labels, name='labels')\n",
    "        auc = tf.metrics.auc(labels=labels, predictions=out, name='auc')\n",
    "        metrics = {\n",
    "            'auc': auc\n",
    "        }\n",
    "        loss = tf.reduce_mean(tf.losses.log_loss(labels=labels, predictions=out))\n",
    "\n",
    "        # ------bulid optimizer------\n",
    "        if params.optimizer == 'Adam':\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=params.learning_rate)\n",
    "        elif params.optimizer == 'Adagrad':\n",
    "            optimizer = tf.train.AdagradOptimizer(learning_rate=params.learning_rate)\n",
    "        elif params.optimizer == 'Momentum':\n",
    "            optimizer = tf.train.MomentumOptimizer(learning_rate=params.learning_rate, momentum=0.95)\n",
    "        elif params.optimizer == 'ftrl':\n",
    "            optimizer = tf.train.FtrlOptimizer(learning_rate=params.learning_rate)\n",
    "        elif params.optimizer == 'Adadelta':\n",
    "            optimizer = tf.train.AdadeltaOptimizer(learning_rate=params.learning_rate)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
    "        else:\n",
    "            train_op = None\n",
    "\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            loss=loss,\n",
    "            eval_metric_ops=metrics,\n",
    "            train_op=train_op)\n",
    "\n",
    "\n",
    "def model_save_pb(params, model):\n",
    "    features = {\n",
    "        'cont_feats': tf.FixedLenFeature(dtype=tf.float32, shape=[params.cont_field_count]),\n",
    "        'cate_feats': tf.FixedLenFeature(dtype=tf.int64, shape=[params.cate_field_count]),\n",
    "    }\n",
    "    if params.multi_feats_type in ['dense', 'sparse2dense']:\n",
    "        for field_name in params.multi_cate_field_list:\n",
    "            features[field_name[0]] = tf.FixedLenFeature(field_name[1], tf.int64)\n",
    "    elif params.multi_feats_type == 'sparse':\n",
    "        for field_name in params.multi_cate_field_list:\n",
    "            features[field_name[0]] = tf.VarLenFeature(tf.int64)\n",
    "\n",
    "    if params.model_pb_type == 'parsing':\n",
    "        serving_input_receiver_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(features)\n",
    "    elif params.model_pb_type == 'raw':\n",
    "        serving_input_receiver_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(features)\n",
    "\n",
    "    return model.export_savedmodel(params.model_pb, serving_input_receiver_fn)\n",
    "\n",
    "\n",
    "def model_save_pb_dssm(params, model):\n",
    "    features = {\n",
    "        'user_cate_feats': tf.FixedLenFeature([params.user_cate_field_count], tf.int64),\n",
    "        'item_cont_feats': tf.FixedLenFeature([params.item_cont_field_count], tf.float32),\n",
    "        'item_cate_feats': tf.FixedLenFeature([params.item_cate_field_count], tf.int64)\n",
    "    }\n",
    "    for field_name in params.multi_cate_field_list:\n",
    "        features[field_name[0]] = tf.VarLenFeature(tf.int64)\n",
    "\n",
    "    if params.model_pb_type == 'parsing':\n",
    "        serving_input_receiver_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(features)\n",
    "    elif params.model_pb_type == 'raw':\n",
    "        serving_input_receiver_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(features)\n",
    "\n",
    "    return model.export_savedmodel(params.model_pb, serving_input_receiver_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "'''\n",
    "功能: 1.通用工具类函数 2.解析配置文件lr.conf&dnn.conf\n",
    "'''\n",
    "import datetime\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def arg_parse(argv):\n",
    "    parse_dict = dict()\n",
    "    for i in range(1, len(argv)):\n",
    "        line_parse = argv[i].split(\"=\")\n",
    "        key = line_parse[0].strip()\n",
    "        value = line_parse[1].strip()\n",
    "        parse_dict[key] = value\n",
    "    return parse_dict\n",
    "\n",
    "\n",
    "def str_list_2_int_list(str_list):\n",
    "    int_list = list()\n",
    "    for i in str_list:\n",
    "        if type(i) == str:\n",
    "            int_list.append(int(i))\n",
    "    return int_list\n",
    "\n",
    "\n",
    "def str_list_2_float_list(str_list):\n",
    "    float_list = list()\n",
    "    for i in str_list:\n",
    "        if type(i) == str:\n",
    "            float_list.append(float(i))\n",
    "    return float_list\n",
    "\n",
    "def parse_feats_conf(conf_path, alg_name):  # 解析配置文件\n",
    "    cont_field_count = 0\n",
    "    cate_field_count = 0\n",
    "    multi_cate_field_count = 0\n",
    "    multi_cate_field_list = list()  # [(multi_cate_feat_name, topN)]\n",
    "    total_field_count = 0\n",
    "    # dssm\n",
    "    user_cont_field_count = 0\n",
    "    user_cate_field_count = 0\n",
    "    item_cont_field_count = 0\n",
    "    item_cate_field_count = 0\n",
    "    # target-attention\n",
    "    target_att_alg_name = ['din', 'dinfm']\n",
    "    target_att_item_single = dict()  # {attention-id: item-index}  single_cate_feats(e.g. item_cat1) was merged into one column named 'cate_feats', so we need record the index\n",
    "    target_att_item = dict()  # {attention-id: item-name} multi_cate_feats(e.g. item_tags) are independent columns, so it is enough to record column names\n",
    "    target_att_user = dict()  # {attention-id: user-name} multi_cate_feats(e.g. user_click_cat1) are independent columns, so it is enough to record column names\n",
    "    target_att_1vN_list = list()  # target-attention: 1-n [[user-name1, item-index1], [user-name2, item-index2], ...]\n",
    "    target_att_NvN_list = list()  # target-attention: n-n [[user-name1, item-name1], [user-name2, item-name2], ...]\n",
    "\n",
    "    files = os.listdir(conf_path)\n",
    "    for file in files:\n",
    "        print (file)\n",
    "        if file != \"dnn.conf\" and file != \"lr.conf\":\n",
    "            continue\n",
    "        file_path = conf_path + \"/\" + file\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                line_data = line.strip()\n",
    "                if line_data == '':\n",
    "                    continue\n",
    "                try:\n",
    "                    config_arr = line_data.split(\"\\t\")\n",
    "                    col_name = config_arr[0]\n",
    "                    new_col_name = config_arr[1]\n",
    "                    if new_col_name == \"none\":\n",
    "                        new_col_name = col_name\n",
    "                    result_type = config_arr[2]\n",
    "\n",
    "                    is_drop = int(config_arr[8])\n",
    "                    att_id = config_arr[10]  # attention_id\n",
    "\n",
    "                    if is_drop == 1:\n",
    "                        continue\n",
    "\n",
    "                    total_field_count += 1\n",
    "\n",
    "                    if result_type == 'arr':  # multi_cate\n",
    "                        func_parse = config_arr[5]  # pre_parse_func = config_arr[4]\n",
    "                        result_parse = config_arr[7]  # result_parse_func = config_arr[6]\n",
    "                        # target-attention\n",
    "                        if alg_name in target_att_alg_name and att_id != '0':\n",
    "                            feat_type = col_name.split('_')[0]\n",
    "                            if feat_type == \"item\":\n",
    "                                target_att_item[att_id] = new_col_name\n",
    "                            else:  # \"user\"\n",
    "                                target_att_user[att_id] = new_col_name\n",
    "\n",
    "                        # multi_cate_feats\n",
    "                        top_n = result_parse if func_parse == 'none' else func_parse\n",
    "                        multi_cate_field_list.append((new_col_name, int(top_n.split(\"=\")[1])))\n",
    "                        multi_cate_field_count += 1\n",
    "\n",
    "                    elif result_type == 'string':  # single_cate\n",
    "                        # target-attention\n",
    "                        if alg_name in target_att_alg_name and att_id != '0':\n",
    "                            feat_type = col_name.split('_')[0]\n",
    "                            if feat_type == \"item\":\n",
    "                                target_att_item_single[att_id] = cate_field_count\n",
    "                        # dssm\n",
    "                        if alg_name == 'dssm':\n",
    "                            feat_type = col_name.split('_')[0]\n",
    "                            if feat_type == \"item\" and '#' not in col_name:  # '#' not in col_name是为了过滤user和item的交叉特征'#'\n",
    "                                item_cate_field_count += 1\n",
    "                            if feat_type == \"user\" and '#' not in col_name:\n",
    "                                user_cate_field_count += 1\n",
    "                        # single_cate_feats\n",
    "                        cate_field_count += 1\n",
    "\n",
    "                    elif result_type == 'float':  # cont\n",
    "                        # dssm cont\n",
    "                        if alg_name == 'dssm':\n",
    "                            feat_type = col_name.split('_')[0]\n",
    "                            if feat_type == \"item\":\n",
    "                                item_cont_field_count += 1\n",
    "                            if feat_type == \"user\":\n",
    "                                user_cont_field_count += 1\n",
    "                        # cont\n",
    "                        cont_field_count += 1\n",
    "                    else:\n",
    "                        print(\"%s is error!!!\" % line_data)\n",
    "                except Exception as e:\n",
    "                    print(\"-----------feat_conf is Error!!!!-----------\")\n",
    "                    print(e)\n",
    "                    print(line_data)\n",
    "                    exit(-1)\n",
    "\n",
    "    parse_feats_dict = dict()\n",
    "    parse_feats_dict[\"cont_field_count\"] = cont_field_count\n",
    "    parse_feats_dict[\"cate_field_count\"] = cate_field_count\n",
    "    parse_feats_dict[\"multi_cate_field_count\"] = multi_cate_field_count\n",
    "    parse_feats_dict[\"multi_cate_field_list\"] = multi_cate_field_list\n",
    "    parse_feats_dict[\"total_field_count\"] = total_field_count\n",
    "    # dssm\n",
    "    parse_feats_dict[\"user_cont_field_count\"] = user_cont_field_count\n",
    "    parse_feats_dict[\"user_cate_field_count\"] = user_cate_field_count\n",
    "    parse_feats_dict[\"item_cont_field_count\"] = item_cont_field_count\n",
    "    parse_feats_dict[\"item_cate_field_count\"] = item_cate_field_count\n",
    "    # target-attention\n",
    "    for k, v in target_att_user.items():\n",
    "        if k in target_att_item_single.keys():\n",
    "            target_att_1vN_list.append((v, target_att_item_single[k]))  # e.g. [(user_click_cat1, 3), ...], '3' means 'item_cat1' is the third feat of 'cate_feats'\n",
    "    parse_feats_dict[\"target_att_1vN_list\"] = target_att_1vN_list\n",
    "    for k, v in target_att_user.items():\n",
    "        if k in target_att_item.keys():\n",
    "            target_att_NvN_list.append((v, target_att_item[k]))  # e.g. [(user_click_tags: item_tags), ...], both are multi_feats\n",
    "    parse_feats_dict[\"target_att_NvN_list\"] = target_att_NvN_list\n",
    "\n",
    "    return parse_feats_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "'''\n",
    "功能：加载tfrecord\n",
    "-------\n",
    "所有连续特征合并为features[\"cont_feats\"], 所有单值离散特征合并为features[\"cate_feats\"], 此时由于位置和特征一一对应, 故必须补齐缺失值, 即数据类型为dense类型->FixedLenFeature\n",
    "多值离散特征不合并, 各自为一维特征(features[\"name1\"], features[\"name2\"], ..., features[\"nameN\"])\n",
    "-------\n",
    "由于多值离散特征fetures[\"name*\"]最终往往会进行sum_pool或mean_pool处理, 故可以不补齐缺失值, 即sparse类型->VarLenFeature, 配合embedding_lookup_sparse使用即可\n",
    "当然多值离散特征也可以像连续特征/单值离散特征一样, 补齐为dense类型->FixedLenFeature, 配合embedding_lookup和reduce_sum/reduce_mean即可\n",
    "但多值离散特征往往维度较大且稀疏, 建议使用sparse方式以节省空间与时间, 除了一些特殊情况(如din)\n",
    "-------\n",
    "关于使用了target-attention的din与dinfm模型, 多值离散特征用target-attention机制代替了简单的sum/mean-pooling, 所以不能使用embedding_lookup_sparse, 即不支持sparse数据\n",
    "这里使用pdded_batch实现了'sparse2dense', 但还是建议在生成tfrecord数据之前直接利用spark补齐缺失值生成dense数据, spark效率更高\n",
    "-------\n",
    "'''\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def parse_example(example, params):\n",
    "    features = {\n",
    "        'label': tf.FixedLenFeature([1], tf.float32),\n",
    "        'cont_feats': tf.FixedLenFeature([params.cont_field_count], tf.float32),\n",
    "        'cate_feats': tf.FixedLenFeature([params.cate_field_count], tf.int64),\n",
    "    }\n",
    "    if params.multi_feats_type == 'dense':\n",
    "        for name_topN in params.multi_cate_field_list:\n",
    "            features[name_topN[0]] = tf.FixedLenFeature([name_topN[1]], tf.int64)\n",
    "    elif params.multi_feats_type in ['sparse', 'sparse2dense']:\n",
    "        for name_topN in params.multi_cate_field_list:\n",
    "            features[name_topN[0]] = tf.VarLenFeature(tf.int64)\n",
    "\n",
    "    parsed_features = tf.parse_single_example(example, features)\n",
    "    label = parsed_features['label']\n",
    "    parsed_features.pop('label')\n",
    "\n",
    "    if params.multi_feats_type == 'sparse2dense':  # 配合padded_batch使用\n",
    "        for name_topN in params.multi_cate_field_list:\n",
    "            parsed_features[name_topN[0]] = tf.sparse_tensor_to_dense(parsed_features[name_topN[0]])\n",
    "\n",
    "    return parsed_features, label\n",
    "\n",
    "\n",
    "def input_fn(data_path, params):\n",
    "    data_set = tf.data.TFRecordDataset(data_path)\n",
    "\n",
    "    if params.multi_feats_type in ['dense', 'sparse']:\n",
    "        data_set = data_set.map(lambda x: parse_example(x, params), num_parallel_calls=params.num_threads) \\\n",
    "            .batch(params.batch_size) \\\n",
    "            .repeat(params.epochs) \\\n",
    "            .prefetch(params.batch_size)\n",
    "\n",
    "    elif params.multi_feats_type == 'sparse2dense':  # 利用padded_batch实现sparse数据转为dense\n",
    "        pad_shapes = dict()\n",
    "        pad_shapes_label = tf.TensorShape([1])\n",
    "        pad_shapes[\"cont_feats\"] = tf.TensorShape([params.cont_field_count])\n",
    "        pad_shapes[\"cate_feats\"] = tf.TensorShape([params.cate_field_count])\n",
    "        for name_topN in params.multi_cate_field_list:\n",
    "            pad_shapes[name_topN[0]] = tf.TensorShape([name_topN[1]])\n",
    "        pad_shapes = (pad_shapes, pad_shapes_label)\n",
    "        data_set = data_set.map(lambda x: parse_example(x, params), num_parallel_calls=params.num_threads) \\\n",
    "            .padded_batch(params.batch_size, padded_shapes=pad_shapes) \\\n",
    "            .repeat(params.epochs) \\\n",
    "            .prefetch(params.batch_size)\n",
    "\n",
    "    else:\n",
    "        print(\"multi_feats_type error!!!\")\n",
    "        exit(-1)\n",
    "\n",
    "    iterator = data_set.make_one_shot_iterator()\n",
    "    feature_dict, label = iterator.get_next()\n",
    "    return feature_dict, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "'''\n",
    "使用target-attention默认数据格式为dense\n",
    "'''\n",
    "import tensorflow as tf\n",
    "\n",
    "def model_fn(labels, features, mode, params):\n",
    "    tf.set_random_seed(2020)\n",
    "    cont_feats = features[\"cont_feats\"]\n",
    "    cate_feats = features[\"cate_feats\"]\n",
    "\n",
    "    #shape=(?, 18),  shape=(?, 33)\n",
    "    print (cont_feats)\n",
    "    print (cate_feats)\n",
    "    \n",
    "    cont_feats_index = tf.Variable([[i for i in range(params.cont_field_count)]], trainable=False, dtype=tf.int64, name=\"cont_feats_index\")\n",
    "    # shape=(1, 18)\n",
    "    cont_feats_index = tf.add(cont_feats_index, params.cate_emb_space_size)\n",
    "\n",
    "    #9000018\n",
    "    feats_size = params.cont_field_count + params.cate_emb_space_size\n",
    "    \n",
    "    # shape=(9000018, 32)\n",
    "    feats_emb = emb_init(name='feats_emb', feat_num=feats_size, embedding_size=params.embedding_size)\n",
    "\n",
    "    \n",
    "    print (cont_feats_index)\n",
    "    print (feats_size)\n",
    "    print (feats_emb)\n",
    "    \n",
    "    # cont_feats\n",
    "    #tensor(\"embedding_lookup:0\", shape=(1, 18, 32), dtype=float32)\n",
    "    #Tensor(\"Reshape:0\", shape=(?, 18, 1), dtype=float32)\n",
    "    #Tensor(\"Mul:0\", shape=(?, 18, 32), dtype=float32)\n",
    "    cont_emb = tf.nn.embedding_lookup(feats_emb, ids=cont_feats_index)  # None * F * embedding_size\n",
    "    cont_value = tf.reshape(cont_feats, shape=[-1, params.cont_field_count, 1])\n",
    "    embeddings = tf.multiply(cont_emb, cont_value)\n",
    "    \n",
    "    print (cont_emb)\n",
    "    print (cont_value)\n",
    "    print (embeddings)\n",
    "    \n",
    "    # cate_feats\n",
    "    #Tensor(\"embedding_lookup_1:0\", shape=(?, 33, 32), dtype=float32)\n",
    "    #Tensor(\"concat_1:0\", shape=(?, 51, 32), dtype=float32)\n",
    "    cate_emb = tf.nn.embedding_lookup(feats_emb, ids=cate_feats)\n",
    "    embeddings = tf.concat([embeddings, cate_emb], axis=1)\n",
    "    \n",
    "    print (cate_emb)\n",
    "    print (embeddings)\n",
    "    \n",
    "    # multi_cate_feats\n",
    "    for name_topN in params.multi_cate_field_list:\n",
    "        multi_cate_emb = tf.nn.embedding_lookup(feats_emb, ids=features[name_topN[0]])  # None, topN, embedding_size\n",
    "        multi_cate_emb = tf.reduce_sum(multi_cate_emb, axis=1)  # None, embedding_size\n",
    "        multi_cate_emb = tf.reshape(multi_cate_emb, shape=[-1, 1, params.embedding_size])\n",
    "        embeddings = tf.concat([embeddings, multi_cate_emb], axis=1)\n",
    "        print ('name_topN ', name_topN)\n",
    "        print (embeddings)\n",
    "    # target-attention 1vN (e.g. item_cat1 & user_click_cat1)\n",
    "    for k_v in params.target_att_1vN_list:\n",
    "        item_feat = tf.split(cate_feats, params.cate_field_count, axis=1)[k_v[1]]\n",
    "        user_feat = features[k_v[0]]\n",
    "        nonzero_len = tf.count_nonzero(user_feat, axis=1)\n",
    "        item_emb = tf.nn.embedding_lookup(feats_emb, ids=item_feat)  # [B, 1, H]\n",
    "        item_emb = tf.reshape(item_emb, shape=[-1, params.embedding_size])  # [B, H]\n",
    "        user_emb = tf.nn.embedding_lookup(feats_emb, ids=user_feat)  # [B, T, H])\n",
    "        att_1vN_emb = attention(item_emb, user_emb, nonzero_len)  # [B, 1, H]\n",
    "        embeddings = tf.concat([embeddings, att_1vN_emb], axis=1)\n",
    "        print ('k_v ', k_v, item_feat)\n",
    "        print (embeddings)\n",
    "    # target-attention NvN (e.g. item_tags & user_click_tags)\n",
    "    for k_v in params.target_att_NvN_list:\n",
    "        item_feat = features[k_v[1]]\n",
    "        user_feat = features[k_v[0]]\n",
    "        nonzero_len = tf.count_nonzero(user_feat, axis=1)\n",
    "        item_emb = tf.nn.embedding_lookup(feats_emb, ids=item_feat)  # [B, N, H]\n",
    "        user_emb = tf.nn.embedding_lookup(feats_emb, ids=user_feat)  # [B, T, H])\n",
    "        att_NvN_emb = attention_multi(item_emb, user_emb, nonzero_len)  # [B, 1, H]\n",
    "        embeddings = tf.concat([embeddings, att_NvN_emb], axis=1)\n",
    "        print ('k_v_1 ', k_v, item_feat)\n",
    "        print (embeddings)\n",
    "        \n",
    "    # deep\n",
    "    embeddings = tf.layers.flatten(embeddings)\n",
    "    len_layers = len(params.hidden_units)\n",
    "    for i in range(0, len_layers):\n",
    "        embeddings = tf.layers.dense(inputs=embeddings, units=params.hidden_units[i], activation=tf.nn.relu)\n",
    "    out = tf.layers.dense(inputs=embeddings, units=1)\n",
    "    score = tf.identity(tf.nn.sigmoid(out), name='score')\n",
    "    model_estimator_spec = model_optimizer(params, mode, labels, score)\n",
    "    return model_estimator_spec\n",
    "\n",
    "\n",
    "def model_estimator(params):\n",
    "    # shutil.rmtree(conf.model_dir, ignore_errors=True)\n",
    "    tf.reset_default_graph()\n",
    "    config = tf.estimator.RunConfig() \\\n",
    "        .replace(\n",
    "        session_config=tf.ConfigProto(device_count={'GPU': params.is_GPU, 'CPU': params.num_threads}, \n",
    "                                      gpu_options=tf.GPUOptions(allow_growth=True),\n",
    "                                      inter_op_parallelism_threads=params.num_threads,\n",
    "                                      intra_op_parallelism_threads=params.num_threads),\n",
    "        log_step_count_steps=params.log_step_count_steps,\n",
    "        save_checkpoints_steps=params.save_checkpoints_steps,\n",
    "        keep_checkpoint_max=params.keep_checkpoint_max,\n",
    "        save_summary_steps=params.save_summary_steps)\n",
    "\n",
    "    model = tf.estimator.Estimator(\n",
    "        model_fn=model_fn,\n",
    "        config=config,\n",
    "        model_dir=params.model_dir,\n",
    "        params=params,\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo-sparse\n",
      "lr.conf\n",
      "dnn.conf\n",
      "demo-dense\n",
      "{'cont_field_count': 18, 'cate_field_count': 33, 'multi_cate_field_count': 15, 'multi_cate_field_list': [('match_pos_tags', 10), ('match_pos_ru_tags', 10), ('match_pos_tag_ext', 15), ('match_pos_ru_tags_ext', 15), ('item_topic_keys', 10), ('item_tags_keys', 10), ('item_tags_ext_keys', 15), ('user_month_cat1_keys', 10), ('user_month_cat2_keys', 10), ('user_month_media_id_keys', 30), ('user_month_tags_keys', 50), ('ruCat1ScoreKeys', 10), ('ruCat2ScoreKeys', 10), ('ruMediaIdScoreKeys', 30), ('ruTagsScoreKeys', 50)], 'total_field_count': 66, 'user_cont_field_count': 0, 'user_cate_field_count': 0, 'item_cont_field_count': 0, 'item_cate_field_count': 0, 'target_att_1vN_list': [('ruCat1ScoreKeys', 2), ('ruCat2ScoreKeys', 3), ('ruMediaIdScoreKeys', 4)], 'target_att_NvN_list': [('ruTagsScoreKeys', 'item_tags_keys')]}\n",
      "-----------check Arguments------------START\n",
      "alg_name:  din\n",
      "task_mode:  train\n",
      "hidden_units:  [512, 256, 256, 128]\n",
      "model_dir:  data/model_save_dir/default\n",
      "model_pb:  data/model_save_pb/default\n",
      "clear_existing_model_dir:  False\n",
      "train_data:  data/tfrecord/demo/demo-sparse\n",
      "eval_data:  data/tfrecord/demo/demo-sparse\n",
      "epochs:  1\n",
      "embedding_size:  32\n",
      "batch_size:  8\n",
      "dropout:  [0.8, 0.8, 0.8, 0.8]\n",
      "optimizer:  Adam\n",
      "learning_rate:  0.001\n",
      "total_field_count:  66\n",
      "cont_field_count:  18\n",
      "cate_field_count:  33\n",
      "multi_cate_field_count:  15\n",
      "multi_cate_field_list('feats_name', topN):  [('match_pos_tags', 10), ('match_pos_ru_tags', 10), ('match_pos_tag_ext', 15), ('match_pos_ru_tags_ext', 15), ('item_topic_keys', 10), ('item_tags_keys', 10), ('item_tags_ext_keys', 15), ('user_month_cat1_keys', 10), ('user_month_cat2_keys', 10), ('user_month_media_id_keys', 30), ('user_month_tags_keys', 50), ('ruCat1ScoreKeys', 10), ('ruCat2ScoreKeys', 10), ('ruMediaIdScoreKeys', 30), ('ruTagsScoreKeys', 50)]\n",
      "multi_feats_type:  sparse2dense\n",
      "target_att_1vN_list('user_feats_name', single_cate_index):  [('ruCat1ScoreKeys', 2), ('ruCat2ScoreKeys', 3), ('ruMediaIdScoreKeys', 4)]\n",
      "target_att_NvN_list('user_feats_name', item_feats_name):  [('ruTagsScoreKeys', 'item_tags_keys')]\n",
      "-----------check Arguments-------------END\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'data/model_save_dir/default', '_tf_random_seed': None, '_save_summary_steps': 500, '_save_checkpoints_steps': 100, '_save_checkpoints_secs': None, '_session_config': device_count {\n",
      "  key: \"CPU\"\n",
      "  value: 1\n",
      "}\n",
      "device_count {\n",
      "  key: \"GPU\"\n",
      "  value: 0\n",
      "}\n",
      "intra_op_parallelism_threads: 1\n",
      "inter_op_parallelism_threads: 1\n",
      "gpu_options {\n",
      "  allow_growth: true\n",
      "}\n",
      ", '_keep_checkpoint_max': 1, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11d465d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Tensor(\"IteratorGetNext:1\", shape=(?, 18), dtype=float32, device=/device:CPU:0)\n",
      "Tensor(\"IteratorGetNext:0\", shape=(?, 33), dtype=int64, device=/device:CPU:0)\n",
      "Tensor(\"Add:0\", shape=(1, 18), dtype=int64)\n",
      "9000018\n",
      "Tensor(\"concat:0\", shape=(9000018, 32), dtype=float32)\n",
      "Tensor(\"embedding_lookup:0\", shape=(1, 18, 32), dtype=float32)\n",
      "Tensor(\"Reshape:0\", shape=(?, 18, 1), dtype=float32)\n",
      "Tensor(\"Mul:0\", shape=(?, 18, 32), dtype=float32)\n",
      "Tensor(\"embedding_lookup_1:0\", shape=(?, 33, 32), dtype=float32)\n",
      "Tensor(\"concat_1:0\", shape=(?, 51, 32), dtype=float32)\n",
      "name_topN  ('match_pos_tags', 10)\n",
      "Tensor(\"concat_2:0\", shape=(?, 52, 32), dtype=float32)\n",
      "name_topN  ('match_pos_ru_tags', 10)\n",
      "Tensor(\"concat_3:0\", shape=(?, 53, 32), dtype=float32)\n",
      "name_topN  ('match_pos_tag_ext', 15)\n",
      "Tensor(\"concat_4:0\", shape=(?, 54, 32), dtype=float32)\n",
      "name_topN  ('match_pos_ru_tags_ext', 15)\n",
      "Tensor(\"concat_5:0\", shape=(?, 55, 32), dtype=float32)\n",
      "name_topN  ('item_topic_keys', 10)\n",
      "Tensor(\"concat_6:0\", shape=(?, 56, 32), dtype=float32)\n",
      "name_topN  ('item_tags_keys', 10)\n",
      "Tensor(\"concat_7:0\", shape=(?, 57, 32), dtype=float32)\n",
      "name_topN  ('item_tags_ext_keys', 15)\n",
      "Tensor(\"concat_8:0\", shape=(?, 58, 32), dtype=float32)\n",
      "name_topN  ('user_month_cat1_keys', 10)\n",
      "Tensor(\"concat_9:0\", shape=(?, 59, 32), dtype=float32)\n",
      "name_topN  ('user_month_cat2_keys', 10)\n",
      "Tensor(\"concat_10:0\", shape=(?, 60, 32), dtype=float32)\n",
      "name_topN  ('user_month_media_id_keys', 30)\n",
      "Tensor(\"concat_11:0\", shape=(?, 61, 32), dtype=float32)\n",
      "name_topN  ('user_month_tags_keys', 50)\n",
      "Tensor(\"concat_12:0\", shape=(?, 62, 32), dtype=float32)\n",
      "name_topN  ('ruCat1ScoreKeys', 10)\n",
      "Tensor(\"concat_13:0\", shape=(?, 63, 32), dtype=float32)\n",
      "name_topN  ('ruCat2ScoreKeys', 10)\n",
      "Tensor(\"concat_14:0\", shape=(?, 64, 32), dtype=float32)\n",
      "name_topN  ('ruMediaIdScoreKeys', 30)\n",
      "Tensor(\"concat_15:0\", shape=(?, 65, 32), dtype=float32)\n",
      "name_topN  ('ruTagsScoreKeys', 50)\n",
      "Tensor(\"concat_16:0\", shape=(?, 66, 32), dtype=float32)\n",
      "k_v  ('ruCat1ScoreKeys', 2) Tensor(\"split:2\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"concat_19:0\", shape=(?, 67, 32), dtype=float32)\n",
      "k_v  ('ruCat2ScoreKeys', 3) Tensor(\"split_1:3\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"concat_22:0\", shape=(?, 68, 32), dtype=float32)\n",
      "k_v  ('ruMediaIdScoreKeys', 4) Tensor(\"split_2:4\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"concat_25:0\", shape=(?, 69, 32), dtype=float32)\n",
      "k_v_1  ('ruTagsScoreKeys', 'item_tags_keys') Tensor(\"IteratorGetNext:3\", shape=(?, 10), dtype=int64, device=/device:CPU:0)\n",
      "Tensor(\"concat_28:0\", shape=(?, 70, 32), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 288000544 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8db8f8fc4d12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-8db8f8fc4d12>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0mmodel_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_save_pb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0meval_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1143\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1171\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[1;32m   1172\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m                                              saving_listeners)\n\u001b[0m\u001b[1;32m   1174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_with_estimator_spec\u001b[0;34m(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\u001b[0m\n\u001b[1;32m   1446\u001b[0m         \u001b[0msave_summaries_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_summary_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         log_step_count_steps=self._config.log_step_count_steps) as mon_sess:\n\u001b[0m\u001b[1;32m   1449\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mMonitoredTrainingSession\u001b[0;34m(master, is_chief, checkpoint_dir, scaffold, hooks, chief_only_hooks, save_checkpoint_secs, save_summaries_steps, save_summaries_secs, config, stop_grace_period_secs, log_step_count_steps, max_wait_secs, save_checkpoint_steps, summary_dir)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0mall_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m   return MonitoredSession(session_creator=session_creator, hooks=all_hooks,\n\u001b[0;32m--> 421\u001b[0;31m                           stop_grace_period_secs=stop_grace_period_secs)\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session_creator, hooks, stop_grace_period_secs)\u001b[0m\n\u001b[1;32m    830\u001b[0m     super(MonitoredSession, self).__init__(\n\u001b[1;32m    831\u001b[0m         \u001b[0msession_creator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_recover\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         stop_grace_period_secs=stop_grace_period_secs)\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session_creator, hooks, should_recover, stop_grace_period_secs)\u001b[0m\n\u001b[1;32m    553\u001b[0m         stop_grace_period_secs=stop_grace_period_secs)\n\u001b[1;32m    554\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_recover\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_RecoverableSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinated_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinated_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sess_creator)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \"\"\"\n\u001b[1;32m   1017\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess_creator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess_creator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1018\u001b[0;31m     \u001b[0m_WrappedSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36m_create_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         logging.info('An error was raised while a session was being created. '\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mcreate_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    710\u001b[0m       \u001b[0;34m\"\"\"Creates a coordinated session.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m       \u001b[0;31m# Keep the tf_sess for unit testing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m       \u001b[0;31m# We don't want coordinator to suppress any exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCoordinator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_stop_exception_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mcreate_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0minit_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scaffold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0minit_feed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scaffold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_feed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m         init_fn=self._scaffold.init_fn)\n\u001b[0m\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/training/session_manager.py\u001b[0m in \u001b[0;36mprepare_session\u001b[0;34m(self, master, init_op, saver, checkpoint_dir, checkpoint_filename_with_path, wait_for_checkpoint, max_wait_secs, config, init_feed_dict, init_fn)\u001b[0m\n\u001b[1;32m    285\u001b[0m                            \"init_fn or local_init_op was given\")\n\u001b[1;32m    286\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minit_op\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minit_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0minit_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "\n",
    "#################### CMD Arguments ####################\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "# ----------------common------------------\n",
    "tf.app.flags.DEFINE_string(\"dt\", \"\", \"date\")\n",
    "tf.app.flags.DEFINE_string(\"alg_name\", \"din\", \"algorithm name\")\n",
    "tf.app.flags.DEFINE_string(\"task_mode\", \"train\", \"task mode type {train, eval, infer, debug}\")\n",
    "tf.app.flags.DEFINE_integer(\"epochs\", 1, \"epochs of training\")\n",
    "tf.app.flags.DEFINE_integer(\"batch_size\", 8, \"Number of batch size\")\n",
    "tf.app.flags.DEFINE_integer(\"embedding_size\", 32, \"Embedding size\")\n",
    "tf.app.flags.DEFINE_integer(\"cate_emb_space_size\", 9000000, \"emb space\")\n",
    "tf.app.flags.DEFINE_string(\"optimizer\", \"Adam\", \"optimizer type {Adam, Adagrad, GD, Momentum}\")\n",
    "tf.app.flags.DEFINE_float(\"learning_rate\", 0.001, \"learning rate\")\n",
    "tf.app.flags.DEFINE_integer(\"learning_rate_decay_steps\", 10000000, \"\")\n",
    "tf.app.flags.DEFINE_float(\"learning_rate_decay_rate\", 0.9, \"\")\n",
    "tf.app.flags.DEFINE_float(\"l2_reg\", 0.01, \"\")\n",
    "tf.app.flags.DEFINE_integer(\"log_step_count_steps\", 100, \"\")\n",
    "tf.app.flags.DEFINE_list(\"hidden_units\", \"512,256,256,128\", \"the layers of dnn\")\n",
    "tf.app.flags.DEFINE_list(\"dropout\", \"0.8,0.8,0.8,0.8\", \"dropout rate\")\n",
    "# ----------------device-----------------\n",
    "tf.app.flags.DEFINE_integer(\"is_GPU\", 0, \"use GPU or not, 1->yes, 0->no\")\n",
    "tf.app.flags.DEFINE_integer(\"num_cpu\", 1, \"Number of CPU\")\n",
    "tf.app.flags.DEFINE_integer(\"num_threads\", 1, \"Number of threads\")\n",
    "# ----------------model_save--------------------\n",
    "tf.app.flags.DEFINE_string(\"model_pb_type\", \"parsing\", \"model_pb export type {raw, parsing}\")\n",
    "tf.app.flags.DEFINE_string(\"model_pb\", \"data/model_save_pb/default\", \"the path for exporting model pb after training\")\n",
    "tf.app.flags.DEFINE_string(\"model_dir\", \"data/model_save_dir/default\", \"the path for saving model checkpoint between training\")\n",
    "tf.app.flags.DEFINE_boolean(\"clear_existing_model_dir\", False, \"clear existing model_dir or not\")\n",
    "tf.app.flags.DEFINE_integer(\"keep_checkpoint_max\", 1, \"\")\n",
    "tf.app.flags.DEFINE_integer(\"save_checkpoints_steps\", 100, \"\")\n",
    "tf.app.flags.DEFINE_integer(\"save_summary_steps\", 500, \"save summary every steps\")\n",
    "# ----------------data(tfrecord)--------------------\n",
    "tf.app.flags.DEFINE_string(\"multi_feats_type\", \"sparse\", \"multi_cate_feats type {dense, sparse}\")  # for notes, see utils/data_load.py\n",
    "tf.app.flags.DEFINE_string(\"train_data\", \"data/tfrecord/demo/demo-sparse\", \"the path of train data\")\n",
    "tf.app.flags.DEFINE_string(\"eval_data\", \"data/tfrecord/demo/demo-sparse\", \"the path of eval data\")\n",
    "# ---------------parse features conf----------------\n",
    "tf.app.flags.DEFINE_string(\"feats_conf\", \"data/tfrecord/demo/\", \"the path of feature config files\")\n",
    "parse_feats_dict = parse_feats_conf(FLAGS.feats_conf, FLAGS.alg_name)\n",
    "print (parse_feats_dict)\n",
    "tf.app.flags.DEFINE_integer(\"total_field_count\", parse_feats_dict[\"total_field_count\"], \"\")\n",
    "tf.app.flags.DEFINE_integer(\"cont_field_count\", parse_feats_dict[\"cont_field_count\"], \"\")\n",
    "tf.app.flags.DEFINE_integer(\"cate_field_count\", parse_feats_dict[\"cate_field_count\"], \"\")\n",
    "tf.app.flags.DEFINE_integer(\"multi_cate_field_count\", parse_feats_dict[\"multi_cate_field_count\"], \"\")\n",
    "tf.app.flags.DEFINE_list(\"multi_cate_field_list\", parse_feats_dict[\"multi_cate_field_list\"], \"\")\n",
    "tf.app.flags.DEFINE_list(\"target_att_1vN_list\", parse_feats_dict[\"target_att_1vN_list\"], \"\")\n",
    "tf.app.flags.DEFINE_list(\"target_att_NvN_list\", parse_feats_dict[\"target_att_NvN_list\"], \"\")\n",
    "tf.app.flags.DEFINE_integer(\"user_cont_field_count\", parse_feats_dict[\"user_cont_field_count\"], \"\")\n",
    "tf.app.flags.DEFINE_integer(\"user_cate_field_count\", parse_feats_dict[\"user_cate_field_count\"], \"\")\n",
    "tf.app.flags.DEFINE_integer(\"item_cont_field_count\", parse_feats_dict[\"item_cont_field_count\"], \"\")\n",
    "tf.app.flags.DEFINE_integer(\"item_cate_field_count\", parse_feats_dict[\"item_cate_field_count\"], \"\")\n",
    "# ------------model special parameters-----------\n",
    "# autoint\n",
    "tf.app.flags.DEFINE_integer(\"autoint_layer_count\", 2, \"\")\n",
    "tf.app.flags.DEFINE_integer(\"autoint_emb_size\", 16, \"\")\n",
    "tf.app.flags.DEFINE_integer(\"autoint_head_count\", 2, \"\")\n",
    "tf.app.flags.DEFINE_integer(\"autoint_use_res\", 1, \"\")\n",
    "\n",
    "\n",
    "def handle_arguments():\n",
    "    # ---common----\n",
    "    FLAGS.hidden_units = str_list_2_int_list(FLAGS.hidden_units)  # python main.py --hidden_units=512,256,128\n",
    "    FLAGS.dropout = str_list_2_float_list(FLAGS.dropout)  # python main.py --dropout=0.8,0.8,0.8\n",
    "    # ---data(tfrecord)---\n",
    "    if FLAGS.alg_name in [\"din\", \"dinfm\"] and FLAGS.multi_feats_type == \"sparse\":  # target-attention函数的输入是dense_tensor, 所以多值离散特征需补齐缺失值, 不支持sparse\n",
    "        FLAGS.multi_feats_type = \"sparse2dense\"\n",
    "\n",
    "\n",
    "def check_arguments():\n",
    "    print(\"-----------check Arguments------------START\")\n",
    "    print(\"alg_name: \", FLAGS.alg_name)\n",
    "    print(\"task_mode: \", FLAGS.task_mode)\n",
    "    print(\"hidden_units: \", FLAGS.hidden_units)\n",
    "    print(\"model_dir: \", FLAGS.model_dir)\n",
    "    print(\"model_pb: \", FLAGS.model_pb)\n",
    "    print(\"clear_existing_model_dir: \", FLAGS.clear_existing_model_dir)\n",
    "    print(\"train_data: \", FLAGS.train_data)\n",
    "    print(\"eval_data: \", FLAGS.eval_data)\n",
    "    print(\"epochs: \", FLAGS.epochs)\n",
    "    print(\"embedding_size: \", FLAGS.embedding_size)\n",
    "    print(\"batch_size: \", FLAGS.batch_size)\n",
    "    print(\"dropout: \", FLAGS.dropout)\n",
    "    print(\"optimizer: \", FLAGS.optimizer)\n",
    "    print(\"learning_rate: \", FLAGS.learning_rate)\n",
    "    print(\"total_field_count: \", FLAGS.total_field_count)\n",
    "    print(\"cont_field_count: \", FLAGS.cont_field_count)\n",
    "    print(\"cate_field_count: \", FLAGS.cate_field_count)\n",
    "    print(\"multi_cate_field_count: \", FLAGS.multi_cate_field_count)\n",
    "    print(\"multi_cate_field_list('feats_name', topN): \", FLAGS.multi_cate_field_list)\n",
    "    print(\"multi_feats_type: \", FLAGS.multi_feats_type)\n",
    "    print(\"target_att_1vN_list('user_feats_name', single_cate_index): \", FLAGS.target_att_1vN_list)\n",
    "    print(\"target_att_NvN_list('user_feats_name', item_feats_name): \", FLAGS.target_att_NvN_list)\n",
    "    print(\"-----------check Arguments-------------END\")\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    handle_arguments()\n",
    "    check_arguments()\n",
    "\n",
    "    if FLAGS.clear_existing_model_dir and FLAGS.task_mode == 'train':\n",
    "        try:\n",
    "            shutil.rmtree(FLAGS.model_dir)\n",
    "        except Exception as e:\n",
    "            print(e, \"at clear_existing_model_dir\")\n",
    "        else:\n",
    "            print(\"existing model cleaned at %s\" % FLAGS.model_dir)\n",
    "\n",
    "    model = model_estimator(FLAGS)\n",
    "\n",
    "    if FLAGS.task_mode == \"train\":\n",
    "        \n",
    "        model.train(input_fn=lambda: input_fn(FLAGS.train_data, FLAGS))\n",
    "        model_op.model_save_pb(FLAGS, model)\n",
    "        eval_result = model.evaluate(input_fn=lambda: data_load.input_fn(FLAGS.eval_data, FLAGS))\n",
    "        print('eval-auc=%.5f\\t loss=%.5f\\t' % (eval_result['auc'], eval_result['loss']))\n",
    "\n",
    "    elif FLAGS.task_mode == \"eval\":\n",
    "        eval_result = model.evaluate(input_fn=lambda: data_load.input_fn(FLAGS.eval_data, FLAGS))\n",
    "        print('eval-auc=%.5f\\t loss=%.5f\\t' % (eval_result['auc'], eval_result['loss']))\n",
    "\n",
    "    elif FLAGS.task_mode == \"infer\":\n",
    "        # preds = model.predict(input_fn=lambda: data_load.input_fn(FLAGS.eval_data, FLAGS), predict_keys=[\"item_embedding\"])\n",
    "        # f = open(FLAGS.test_data, \"rb\")\n",
    "        # with open(FLAGS.infer_result, \"w\") as fo:\n",
    "        #     for line, p in zip(f, preds):\n",
    "        #         id = line.decode(\"utf-8\").split(\" \")[10]\n",
    "        #         # print(p)\n",
    "        #         # fo.write(\"%f\\n\" % (prob[\"embedding\"]))\n",
    "        #         emb = ','.join([\"%.6f\" % f for f in list(p[\"item_embedding\"])])\n",
    "        #         fo.write(id + \"|\" + id + \"|\" + emb + \"\\n\")\n",
    "        pass\n",
    "\n",
    "    elif FLAGS.task_mode == \"debug\":  # make some fake data for debugging !!!开发中，暂时不可用\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        print(\"task_mode Error!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
