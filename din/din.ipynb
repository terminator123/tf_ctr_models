{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#参考 https://blog.csdn.net/Blank_spaces/article/details/106398162?utm_medium=distribute.pc_relevant.none-task-blog-OPENSEARCH-4.control&depth_1-utm_source=distribute.pc_relevant.none-task-blog-OPENSEARCH-4.control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据集下载\n",
    "# !wget -c http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_5.json.gz\n",
    "# !gzip -d reviews_Electronics_5.json.gz\n",
    "# !wget -c http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/meta_Electronics.json.gz\n",
    "# !gzip -d meta_Electronics.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#review是用户的行为数据 meta是广告数据\n",
    "review_sample = {\n",
    "  \"reviewerID\": \"A2SUAM1J3GNN3B\", #用户id\n",
    "  \"asin\": \"0000013714\",\n",
    "  \"reviewerName\": \"J. McDonald\",\n",
    "  \"helpful\": [2, 3],\n",
    "  \"reviewText\": \"I bought this for my husband who plays the piano.  He is having a wonderful time playing these old hymns.  The music  is at times hard to read because we think the book was published for singing from more than playing from.  Great purchase though!\",\n",
    "  \"overall\": 5.0,\n",
    "  \"summary\": \"Heavenly Highway Hymns\",\n",
    "  \"unixReviewTime\": 1252800000, #时间戳\n",
    "  \"reviewTime\": \"09 13, 2009\"\n",
    "}\n",
    "meta_sample = {\n",
    "  \"asin\": \"0000031852\", #物品id\n",
    "  \"title\": \"Girls Ballet Tutu Zebra Hot Pink\",\n",
    "  \"price\": 3.17,\n",
    "  \"imUrl\": \"http://ecx.images-amazon.com/images/I/51fAmVkTbyL._SY300_.jpg\",\n",
    "  \"related\":\n",
    "  {\n",
    "    \"also_bought\": [\"B00JHONN1S\", \"B002BZX8Z6\", \"B00D2K1M3O\", \"0000031909\", \"B00613WDTQ\", \"B00D0WDS9A\", \"B00D0GCI8S\", \"0000031895\", \"B003AVKOP2\", \"B003AVEU6G\", \"B003IEDM9Q\", \"B002R0FA24\", \"B00D23MC6W\", \"B00D2K0PA0\", \"B00538F5OK\", \"B00CEV86I6\", \"B002R0FABA\", \"B00D10CLVW\", \"B003AVNY6I\", \"B002GZGI4E\", \"B001T9NUFS\", \"B002R0F7FE\", \"B00E1YRI4C\", \"B008UBQZKU\", \"B00D103F8U\", \"B007R2RM8W\"],\n",
    "    \"also_viewed\": [\"B002BZX8Z6\", \"B00JHONN1S\", \"B008F0SU0Y\", \"B00D23MC6W\", \"B00AFDOPDA\", \"B00E1YRI4C\", \"B002GZGI4E\", \"B003AVKOP2\", \"B00D9C1WBM\", \"B00CEV8366\", \"B00CEUX0D8\", \"B0079ME3KU\", \"B00CEUWY8K\", \"B004FOEEHC\", \"0000031895\", \"B00BC4GY9Y\", \"B003XRKA7A\", \"B00K18LKX2\", \"B00EM7KAG6\", \"B00AMQ17JA\", \"B00D9C32NI\", \"B002C3Y6WG\", \"B00JLL4L5Y\", \"B003AVNY6I\", \"B008UBQZKU\", \"B00D0WDS9A\", \"B00613WDTQ\", \"B00538F5OK\", \"B005C4Y4F6\", \"B004LHZ1NY\", \"B00CPHX76U\", \"B00CEUWUZC\", \"B00IJVASUE\", \"B00GOR07RE\", \"B00J2GTM0W\", \"B00JHNSNSM\", \"B003IEDM9Q\", \"B00CYBU84G\", \"B008VV8NSQ\", \"B00CYBULSO\", \"B00I2UHSZA\", \"B005F50FXC\", \"B007LCQI3S\", \"B00DP68AVW\", \"B009RXWNSI\", \"B003AVEU6G\", \"B00HSOJB9M\", \"B00EHAGZNA\", \"B0046W9T8C\", \"B00E79VW6Q\", \"B00D10CLVW\", \"B00B0AVO54\", \"B00E95LC8Q\", \"B00GOR92SO\", \"B007ZN5Y56\", \"B00AL2569W\", \"B00B608000\", \"B008F0SMUC\", \"B00BFXLZ8M\"],\n",
    "    \"bought_together\": [\"B002BZX8Z6\"]\n",
    "  },\n",
    "  \"salesRank\": {\"Toys & Games\": 211836},\n",
    "  \"brand\": \"Coxlures\",\n",
    "  \"categories\": [[\"Sports & Outdoors\", \"Other Sports\", \"Dance\"]] #种类列表\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json格式转为pickle格式\n",
    "import pickle\n",
    "import pandas as pd\n",
    "def to_df(file_path):\n",
    "    with open(file_path, 'r') as fin:\n",
    "        df = {}\n",
    "        i = 0\n",
    "        for line in fin:\n",
    "            df[i] = eval(line)\n",
    "            i += 1\n",
    "        df = pd.DataFrame.from_dict(df, orient='index')\n",
    "        return df\n",
    "reviews_df = to_df(\"./reviews_Electronics_5.json\")\n",
    "\n",
    "with open(\"reviews.pkl\", 'wb') as f:\n",
    "    pickle.dump(reviews_df, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "meta_df = to_df('./meta_Electronics.json')\n",
    "meta_df = meta_df[meta_df['asin'].isin(reviews_df['asin'].unique())]\n",
    "meta_df = meta_df.reset_index(drop=True)\n",
    "with open('meta.pkl', 'wb') as f:\n",
    "    pickle.dump(meta_df, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对revies和meta数据进行处理\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "random.seed(2020)\n",
    "def build_map(df, col_name):\n",
    "    key = sorted(df[col_name].unique().tolist())\n",
    "    m = dict(zip(key, range(len(key))))\n",
    "    df[col_name] = df[col_name].map(lambda x: m[x])\n",
    "    return m, key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = pd.read_pickle('./reviews.pkl')\n",
    "#评论只抽取用户id, 商品id, 时间戳\n",
    "review_df = reviews_df[['reviewerID', 'asin', 'unixReviewTime']]\n",
    "meta_df = pd.read_pickle('./meta.pkl')\n",
    "meta_df = meta_df[['asin', 'categories']]\n",
    "meta_df['categories'] = meta_df['categories'].map(lambda x: x[-1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meta_df的物品id映射\n",
    "asin_map, asin_key = build_map(meta_df, 'asin')\n",
    "cate_map, cate_key = build_map(meta_df, 'categories')\n",
    "revi_map, revi_key = build_map(reviews_df, 'reviewerID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192403\n"
     ]
    }
   ],
   "source": [
    "user_count, item_count, cate_count, example_count = len(revi_map), len(asin_map), len(cate_map), reviews_df.shape[0]\n",
    "print (user_count)\n",
    "#按物品id排序，并重置索引\n",
    "meta_df = meta_df.sort_values('asin')\n",
    "meta_df = meta_df.reset_index(drop=True)\n",
    "#review_df文件物品id进行映射，并按照用户id、浏览时间进行排序，充值索引\n",
    "reviews_df['asin'] = review_df['asin'].map(lambda x: asin_map[x])\n",
    "reviews_df = reviews_df.sort_values(['reviewerID', 'unixReviewTime'])\n",
    "reviews_df = reviews_df.reset_index(drop=True)\n",
    "reviews_df = reviews_df[['reviewerID', 'asin', 'unixReviewTime']]\n",
    "#各个物品对应的类别\n",
    "cate_list = np.array(meta_df['categories'], dtype='int32')\n",
    "\n",
    "#保存所需数据为pkl文件\n",
    "with open('./remap.pkl', 'wb') as f:\n",
    "    pickle.dump(reviews_df, f, pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump(cate_list, f, pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump((user_count, item_count, cate_count, example_count), f, pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump((asin_key, cate_key, revi_key), f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192403\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "t = (reviews_df['reviewerID'].unique().tolist())\n",
    "print (len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384806 192403\n"
     ]
    }
   ],
   "source": [
    "with open('remap.pkl', 'rb') as f:\n",
    "    reviews_df = pickle.load(f)\n",
    "    cate_list = pickle.load(f)\n",
    "    user_count, item_count, cate_count, example_count = pickle.load(f)\n",
    "\n",
    "train_set, test_set = [], []\n",
    "max_sl = 0    \n",
    "for reviewerID, hist in reviews_df.groupby('reviewerID'):\n",
    "    # 每个用户浏览过的物品，即为正样本\n",
    "    pos_list = hist['asin'].tolist()\n",
    "    max_sl = max(max_sl, len(pos_list))\n",
    "    # 生成负样本\n",
    "    def gen_neg():\n",
    "        neg = pos_list[0]\n",
    "        while neg in pos_list:\n",
    "            neg = random.randint(0, item_count - 1)\n",
    "        return neg\n",
    "\n",
    "    # 正负样本比例1：1\n",
    "    neg_list = [gen_neg() for i in range(len(pos_list))]\n",
    "\n",
    "    for i in range(1, len(pos_list)):\n",
    "        # 生成每一次的历史记录，即之前的浏览历史\n",
    "        hist = pos_list[:i]\n",
    "        sl = len(hist)\n",
    "        if i != len(pos_list) - 1:\n",
    "            # 保存正负样本，格式：用户ID，正/负物品id，浏览历史，浏览历史长度，标签（1/0）\n",
    "            train_set.append((reviewerID, pos_list[i], hist, sl, 1))\n",
    "            train_set.append((reviewerID, neg_list[i], hist, sl, 0))\n",
    "        else:\n",
    "            # 最后一次保存为测试集\n",
    "            test_set.append((reviewerID, pos_list[i], hist, sl, 1))\n",
    "            test_set.append((reviewerID, neg_list[i], hist, sl, 0))\n",
    "\n",
    "# 打乱顺序\n",
    "random.shuffle(train_set)\n",
    "random.shuffle(test_set)\n",
    "print (len(test_set), user_count)\n",
    "assert len(test_set) == 2 * user_count\n",
    "# 写入dataset.pkl文件\n",
    "with open('./dataset.pkl', 'wb') as f:\n",
    "    pickle.dump(train_set, f, pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump(test_set, f, pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump(cate_list, f, pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump((user_count, item_count, cate_count, max_sl), f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class DIN(tf.keras.Model):\n",
    "    def __init__(self, user_num, item_num, cate_num, cate_list, hidden_units):\n",
    "        \"\"\"\n",
    "        user_num: 用户数量\n",
    "        item_num: 物品数量\n",
    "        cate_num: 物品种类数量\n",
    "        cate_list: 物品种类列表\n",
    "        hidden_units: 隐藏层单元\n",
    "        \"\"\"\n",
    "        super(DIN, self).__init__()\n",
    "        self.cate_list = tf.convert_to_tensor(cate_list, dtype=tf.int32)\n",
    "        self.hidden_units = hidden_units\n",
    "        self.item_embed = tf.keras.layers.Embedding(\n",
    "            input_dim = item_num, output_dim=self.hidden_units, embeddings_initializer='random_uniform',\n",
    "            embeddings_regularizer = tf.keras.regularizers.l2(0.01), name='item_embed'\n",
    "            )\n",
    "        self.cate_embed = tf.keras.layers.Embedding(\n",
    "            input_dim = cate_num, output_dim=self.hidden_units, embeddings_initializer='random_uniform',\n",
    "            embeddings_regularizer = tf.keras.regularizers.l2(0.01), name='cate_embed'\n",
    "            )\n",
    "        self.dense = tf.keras.layers.Dense(self.hidden_units)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.concat = tf.keras.layers.Concatenate(axis=-1)\n",
    "        self.att_dense1 = tf.keras.layers.Dense(80, activation='sigmoid')\n",
    "        self.att_dense2 = tf.keras.layers.Dense(40, activation='sigmoid')\n",
    "        self.att_dense3 = tf.keras.layers.Dense(1)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.concat2 = tf.keras.layers.Concatenate(axis=-1)\n",
    "        self.dense1 = tf.keras.layers.Dense(80, activation='sigmoid')\n",
    "        self.activation1 = tf.keras.layers.PReLU()\n",
    "        # self.activation1 = Dice()\n",
    "        self.dense2 = tf.keras.layers.Dense(40, activation='sigmoid')\n",
    "        self.activation2 = tf.keras.layers.PReLU()\n",
    "        # self.activation2 = Dice()\n",
    "        self.dense3 = tf.keras.layers.Dense(1, activation=None)\n",
    "    def call(self, inputs):\n",
    "        #user为用户id， item为物品id， hist为之前的历史记录，即物品id列表，sl为最大列表长度\n",
    "        user, item, hist, sl = inputs[0], tf.squeeze(inputs[1], axis=1), inputs[2], tf.squeeze(inputs[3], axis=1)\n",
    "        item_embed = self.concat_embed(item)\n",
    "        print (item_embed)\n",
    "        hist_embed = self.concat_embed(hist)\n",
    "        print (hist_embed)\n",
    "        #经过attention的物品embedding\n",
    "        hist_att_embed = self.attention(item_embed, hist_embed, sl)\n",
    "        hist_att_embed = self.bn1(hist_att_embed)\n",
    "        hist_att_embed = tf.reshape(hist_att_embed, [-1, self.hidden_units * 2])\n",
    "        u_embed = self.dense(hist_att_embed)\n",
    "        item_embed = tf.reshape(item_embed, [-1, item_embed.shape[-1]])\n",
    "        #联合用户行为embedding， 候选物品embedding\n",
    "        embed = self.concat2([u_embed, item_embed])\n",
    "        #mlp\n",
    "        x = self.bn2(embed)\n",
    "        x = self.dense1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.dense3(x)\n",
    "        outputs = tf.nn.sigmoid(x)\n",
    "        return outputs\n",
    "        \n",
    "    def concat_embed(self, item):\n",
    "        #拼接物品embedding和物品种类embedding\n",
    "        #input : 物品id\n",
    "        #return: 拼接后的embedding\n",
    "        cate = tf.gather(self.cate_list, item)\n",
    "        cate = tf.squeuze(cate, axis=1) if cate.shape[-1] == 1 else cate\n",
    "        item_embed = self.item_embed(item)\n",
    "        item_cate_embed = self.cate_embed(cate)\n",
    "        embed = self.concat([item_embed, item_cate_embed])\n",
    "        return embed\n",
    "    def attention(self, queries, keys, keys_length):\n",
    "        \"\"\"\n",
    "        activate unit\n",
    "        params queries: 候选广告（物品）embedding\n",
    "        param keys: 用户行为（历史记录） embedding\n",
    "        param keys_length: 用户行为embedding中的有效长度\n",
    "        return\n",
    "        \"\"\"\n",
    "        #候选物品的隐藏向量维度 hidden_unit * 2\n",
    "        queries_hidden_units = queries.shape[-1]\n",
    "        #每个历史记录的物品的embed都要与候选物品的embed拼接，顾候选物品embed重复keys.shape[1]次\n",
    "        #keys.shape[1]为最大的序列长度，即431, 为了方便矩阵计算\n",
    "        # [None, 431 * hidden_unit * 2]\n",
    "        queries = tf.tile(queries, [1, keys.shape[1]])\n",
    "        #重塑候选物品embed的shape\n",
    "        # [None, 431, hidden_unit * 2]\n",
    "        queries = tf.reshape(queries, [-1, keys.shape[1], queries_hidden_units])\n",
    "        #拼接候选物品embed与hist物品embed\n",
    "        # [None, 431 * 2 * 4]\n",
    "        embed = tf.concat([queries, keys, queries - keys, queries * keys], axis=-1)\n",
    "        #全连接， 得到权重w\n",
    "        d_layer_1 = self.att_dense1(embed)\n",
    "        d_layer_2 = self.att_dense2(d_layer_1)\n",
    "        #[None, 431, 1]\n",
    "        d_layer_3 = self.att_dense3(d_layer_2)\n",
    "        #重塑输出权重类型，每个hist物品embed有对应权重值\n",
    "        # [None, 1, 431]\n",
    "        outputs = tf.reshape(d_layer_3, [-1, 1, keys.shape[1]])\n",
    "        #Mask\n",
    "        # 此处将历史记录的物品embed令为True\n",
    "        # [None, 431]\n",
    "        key_masks = tf.sequence_mask(keys_length, keys.shape[1])\n",
    "        # 增添维度\n",
    "        # [None, 1, 431]\n",
    "        key_masks = tf.expand_dims(key_masks, 1)\n",
    "        # 填充矩阵\n",
    "        paddings = tf.ones_like(outputs) * (-2 ** 32 + 1)\n",
    "        # 构造输出矩阵， 其实就是为了实现[sum_pooling]， true即为原outputs的值， False为上述填充值，为很小的值，softmax后接近0\n",
    "        # [None, 1, 431] 每个历史浏览物品的权重\n",
    "        outputs = tf.where(key_masks, outputs, paddings)\n",
    "        # scale， keys.shape[-1]为hist_embed的隐藏单元数\n",
    "        outputs = outputs / (keys.shape[-1] ** 0.5)\n",
    "        # Activation, 归一化\n",
    "        outputs = tf.nn.softmax(outputs)\n",
    "        # 对hist_embed进行加权\n",
    "        # [None, 1, 431] * [None, 431, hidden_unit * 2] = [None, 1, hidden_unit * 2]\n",
    "        outputs = tf.matmul(outputs, keys)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_data(dataset, max_sl):\n",
    "    user = np.array(dataset[:, 0], dtype='int32')\n",
    "    item = np.array(dataset[:, 1], dtype='int32')\n",
    "    hist = dataset[:, 2]\n",
    "    hist_matrix = tf.keras.preprocessing.sequence.pad_sequences(hist, maxlen=max_sl, padding='post')\n",
    "    sl = np.array(dataset[:, 3], dtype='int32')\n",
    "    y = np.array(dataset[:, 4], dtype='float32')\n",
    "    return user, item, hist_matrix, sl, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from time import time\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import AUC\n",
    "import os\n",
    "\n",
    "file = 'raw_data/remap.pkl'\n",
    "maxlen = 20\n",
    "embed_dim = 8\n",
    "att_hidden_units = [80, 40]\n",
    "ffn_hidden_units = 64\n",
    "dnn_dropout = 0.5\n",
    "att_activation = 'sigmoid'\n",
    "ffn_activation = 'prelu'\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 8\n",
    "epochs = 1\n",
    "#========create dataset============\n",
    "with open('dataset.pkl', 'rb') as f:\n",
    "    train_set = pickle.load(f)\n",
    "    test_set = pickle.load(f)\n",
    "    cate_list = pickle.load(f)\n",
    "    user_count, item_count, cate_count, max_sl = pickle.load(f)\n",
    "    train_inputs = input_data(np.array(train_set), max_sl)\n",
    "    test_inputs = input_data(np.array(test_set), max_sl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"din_4/concatenate_8/concat:0\", shape=(None, 128), dtype=float32)\n",
      "Tensor(\"din_4/concatenate_8/concat_1:0\", shape=(None, 431, 128), dtype=float32)\n",
      "Tensor(\"din_4/concatenate_8/concat:0\", shape=(None, 128), dtype=float32)\n",
      "Tensor(\"din_4/concatenate_8/concat_1:0\", shape=(None, 431, 128), dtype=float32)\n",
      " 24223/326096 [=>............................] - ETA: 1:15:39 - loss: 0.8952 - auc_4: 0.6400"
     ]
    }
   ],
   "source": [
    "#=======build model===============\n",
    "model = DIN(user_count, item_count, cate_count, cate_list, ffn_hidden_units)\n",
    "#model.summary()\n",
    "# ============================model checkpoint======================\n",
    "# check_path = 'save/din_weights.epoch_{epoch:04d}.val_loss_{val_loss:.4f}.ckpt'\n",
    "# checkpoint = tf.keras.callbacks.ModelCheckpoint(check_path, save_weights_only=True,\n",
    "#                                                 verbose=1, period=5)\n",
    "# =========================Compile============================\n",
    "model.compile(loss=binary_crossentropy, optimizer=Adam(learning_rate=learning_rate), metrics=[AUC()])\n",
    "# ===========================Fit==============================\n",
    "model.fit(\n",
    "    train_inputs[0:4],\n",
    "    train_inputs[4],\n",
    "    epochs=epochs,\n",
    "    # callbacks=[EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)],  # checkpoint\n",
    "    validation_data=(test_inputs[0:3], test_inputs[3]),\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "# ===========================Test==============================\n",
    "print('test AUC: %f' % model.evaluate(test_X, test_y, batch_size=batch_size)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:chenqun-python36_tf2.0]",
   "language": "python",
   "name": "conda-env-chenqun-python36_tf2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
